<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html xmlns:erl="http://erlang.org" xmlns:fn="http://www.w3.org/2005/02/xpath-functions">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../../otp_doc.css" type="text/css">
<title>Erlang -- Thread Progress</title>
</head>
<body>
<div id="container">
<script id="js" type="text/javascript" language="JavaScript" src="../../js/flipmenu/flipmenu.js"></script><script id="js2" type="text/javascript" src="../../js/erlresolvelinks.js"></script><script language="JavaScript" type="text/javascript">
            <!--
              function getWinHeight() {
                var myHeight = 0;
                if( typeof( window.innerHeight ) == 'number' ) {
                  //Non-IE
                  myHeight = window.innerHeight;
                } else if( document.documentElement && ( document.documentElement.clientWidth ||
                                                         document.documentElement.clientHeight ) ) {
                  //IE 6+ in 'standards compliant mode'
                  myHeight = document.documentElement.clientHeight;
                } else if( document.body && ( document.body.clientWidth || document.body.clientHeight ) ) {
                  //IE 4 compatible
                  myHeight = document.body.clientHeight;
                }
                return myHeight;
              }

              function setscrollpos() {
                var objf=document.getElementById('loadscrollpos');
                 document.getElementById("leftnav").scrollTop = objf.offsetTop - getWinHeight()/2;
              }

              function addEvent(obj, evType, fn){
                if (obj.addEventListener){
                obj.addEventListener(evType, fn, true);
                return true;
              } else if (obj.attachEvent){
                var r = obj.attachEvent("on"+evType, fn);
                return r;
              } else {
                return false;
              }
             }

             addEvent(window, 'load', setscrollpos);

             //--></script><div id="leftnav"><div class="innertube">
<div class="erlang-logo-wrapper"><a href="../../index.html"><img alt="Erlang Logo" src="../../erlang-logo.png" class="erlang-logo"></a></div>
<p class="section-title">Erlang Run-Time System Application (ERTS)</p>
<p class="section-subtitle">Internal Documentation</p>
<p class="section-version">Version 10.7.2.19</p>
<ul class="panel-sections">
<li><a href="users_guide.html">User's Guide</a></li>
<li><a href="index.html">Reference Manual</a></li>
<li><a href="internal_docs.html">Internal Documentation</a></li>
<li><a href="release_notes.html">Release Notes</a></li>
<li><a href="erts.pdf">PDF</a></li>
<li><a href="../../index.html">Top</a></li>
</ul>
<ul class="expand-collapse-items">
<li><a href="javascript:openAllFlips()">Expand All</a></li>
<li><a href="javascript:closeAllFlips()">Contract All</a></li>
</ul>
<h3>Chapters</h3>
<ul class="flipMenu" imagePath="../../js/flipmenu">
<li id="no" title="Carrier Migration" expanded="false">Carrier Migration<ul>
<li><a href="CarrierMigration.html">
              Top of chapter
            </a></li>
<li title="Introduction"><a href="CarrierMigration.html#introduction">Introduction</a></li>
<li title="Problem"><a href="CarrierMigration.html#problem">Problem</a></li>
<li title="Solution"><a href="CarrierMigration.html#solution">Solution</a></li>
</ul>
</li>
<li id="loadscrollpos" title="Thread Progress" expanded="true">Thread Progress<ul>
<li><a href="ThreadProgress.html">
              Top of chapter
            </a></li>
<li title="Problems"><a href="ThreadProgress.html#problems">Problems</a></li>
<li title="Functionality Used to Address These Problems"><a href="ThreadProgress.html#functionality-used-to-address-these-problems">Functionality Used to Address These Problems</a></li>
<li title="Implementation of the Thread Progress Functionality"><a href="ThreadProgress.html#implementation-of-the-thread-progress-functionality">Implementation of the Thread Progress Functionality</a></li>
</ul>
</li>
<li id="no" title="Non-Blocking Code Loading" expanded="false">Non-Blocking Code Loading<ul>
<li><a href="CodeLoading.html">
              Top of chapter
            </a></li>
<li title="Introduction"><a href="CodeLoading.html#introduction">Introduction</a></li>
<li title="The Load Phases"><a href="CodeLoading.html#the-load-phases">The Load Phases</a></li>
<li title="The Finishing Sequence"><a href="CodeLoading.html#the-finishing-sequence">The Finishing Sequence</a></li>
</ul>
</li>
<li id="no" title="Non-blocking trace setting" expanded="false">Non-blocking trace setting<ul>
<li><a href="Tracing.html">
              Top of chapter
            </a></li>
<li title="Introduction"><a href="Tracing.html#introduction">Introduction</a></li>
<li title="Redesign of Breakpoint Wheel"><a href="Tracing.html#redesign-of-breakpoint-wheel">Redesign of Breakpoint Wheel</a></li>
<li title="Same Same but Different"><a href="Tracing.html#same-same-but-different">Same Same but Different</a></li>
<li title="Adding a new Breakpoint"><a href="Tracing.html#adding-a-new-breakpoint">Adding a new Breakpoint</a></li>
<li title="To Updating and Remove Breakpoints"><a href="Tracing.html#to-updating-and-remove-breakpoints">To Updating and Remove Breakpoints</a></li>
<li title="Global Tracing"><a href="Tracing.html#global-tracing">Global Tracing</a></li>
<li title="Future work"><a href="Tracing.html#future-work">Future work</a></li>
</ul>
</li>
<li id="no" title="Delayed Dealloc" expanded="false">Delayed Dealloc<ul>
<li><a href="DelayedDealloc.html">
              Top of chapter
            </a></li>
<li title="Problem"><a href="DelayedDealloc.html#problem">Problem</a></li>
<li title="Functionality Used to Address This problem"><a href="DelayedDealloc.html#functionality-used-to-address-this-problem">Functionality Used to Address This problem</a></li>
</ul>
</li>
<li id="no" title="The beam_makeops script" expanded="false">The beam_makeops script<ul>
<li><a href="beam_makeops.html">
              Top of chapter
            </a></li>
<li title="Introduction"><a href="beam_makeops.html#introduction">Introduction</a></li>
<li title="An example: the move instruction"><a href="beam_makeops.html#an-example--the-move-instruction">An example: the move instruction</a></li>
<li title="Short overview of instruction loading"><a href="beam_makeops.html#short-overview-of-instruction-loading">Short overview of instruction loading</a></li>
<li title="Running beam_makeops"><a href="beam_makeops.html#running-beam_makeops">Running beam_makeops</a></li>
<li title="Syntax of .tab files"><a href="beam_makeops.html#syntax-of-.tab-files">Syntax of .tab files</a></li>
</ul>
</li>
<li id="no" title="Counting Instructions" expanded="false">Counting Instructions<ul><li><a href="CountingInstructions.html">
              Top of chapter
            </a></li></ul>
</li>
<li id="no" title="Erlang Garbage Collector" expanded="false">Erlang Garbage Collector<ul>
<li><a href="GarbageCollection.html">
              Top of chapter
            </a></li>
<li title="Overview"><a href="GarbageCollection.html#overview">Overview</a></li>
<li title="Generational Garbage Collection"><a href="GarbageCollection.html#generational-garbage-collection">Generational Garbage Collection</a></li>
<li title="The young heap"><a href="GarbageCollection.html#the-young-heap">The young heap</a></li>
<li title="Sizing the heap"><a href="GarbageCollection.html#sizing-the-heap">Sizing the heap</a></li>
<li title="Literals"><a href="GarbageCollection.html#literals">Literals</a></li>
<li title="Binary heap"><a href="GarbageCollection.html#binary-heap">Binary heap</a></li>
<li title="Messages"><a href="GarbageCollection.html#messages">Messages</a></li>
<li title="References"><a href="GarbageCollection.html#references">References</a></li>
</ul>
</li>
<li id="no" title="Process and Port Tables" expanded="false">Process and Port Tables<ul>
<li><a href="PTables.html">
              Top of chapter
            </a></li>
<li title="Problems"><a href="PTables.html#problems">Problems</a></li>
<li title="Solution"><a href="PTables.html#solution">Solution</a></li>
</ul>
</li>
<li id="no" title="Port Signals" expanded="false">Port Signals<ul>
<li><a href="PortSignals.html">
              Top of chapter
            </a></li>
<li title="Problems"><a href="PortSignals.html#problems">Problems</a></li>
<li title="Solution"><a href="PortSignals.html#solution">Solution</a></li>
</ul>
</li>
<li id="no" title="Process Management Optimizations" expanded="false">Process Management Optimizations<ul>
<li><a href="ProcessManagementOptimizations.html">
              Top of chapter
            </a></li>
<li title="Problems"><a href="ProcessManagementOptimizations.html#problems">Problems</a></li>
<li title="Solution"><a href="ProcessManagementOptimizations.html#solution">Solution</a></li>
</ul>
</li>
<li id="no" title="Super Carrier" expanded="false">Super Carrier<ul>
<li><a href="SuperCarrier.html">
              Top of chapter
            </a></li>
<li title="Problem"><a href="SuperCarrier.html#problem">Problem</a></li>
<li title="Solution"><a href="SuperCarrier.html#solution">Solution</a></li>
</ul>
</li>
</ul>
</div></div>
<div id="content">
<div class="innertube">
<h1>2 Thread Progress</h1>


<h3>
<a name="Problems"></a><span onMouseOver="document.getElementById('ghlink-problems-idm30050').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-problems-idm30050').style.visibility = 'hidden';"><span id="ghlink-problems-idm30050"></span><a class="title_link" name="problems" href="#problems">2.1 
          Problems</a></span>
</h3>



<h4>
<a name="Problems_Knowing-When-Threads-Have-Completed-Accesses-to-a-Data-Structure"></a><span onMouseOver="document.getElementById('ghlink-knowing-when-threads-have-completed-accesses-to-a-data-structure-idm30053').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-knowing-when-threads-have-completed-accesses-to-a-data-structure-idm30053').style.visibility = 'hidden';"><span id="ghlink-knowing-when-threads-have-completed-accesses-to-a-data-structure-idm30053"></span><a class="title_link" name="knowing-when-threads-have-completed-accesses-to-a-data-structure" href="#knowing-when-threads-have-completed-accesses-to-a-data-structure">Knowing When Threads Have Completed Accesses to a Data Structure</a></span>
</h4>



<p>
When multiple threads access the same data structure you often need to
know when all threads have completed their accesses. For example, in
order to know when it is safe to deallocate the data structure. One
simple way to accomplish this is to reference count all accesses to
the data structure. The problem with this approach is that the cache
line where the reference counter is located needs to be communicated
between all involved processors. Such communication can become
extremely expensive and will scale poorly if the reference counter is
frequently accessed. That is, we want to use some other approach of
keeping track of threads than reference counting.
</p>



<h4>
<a name="Problems_Knowing-That-Modifications-of-Memory-is-Consistently-Observed"></a><span onMouseOver="document.getElementById('ghlink-knowing-that-modifications-of-memory-is-consistently-observed-idm30057').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-knowing-that-modifications-of-memory-is-consistently-observed-idm30057').style.visibility = 'hidden';"><span id="ghlink-knowing-that-modifications-of-memory-is-consistently-observed-idm30057"></span><a class="title_link" name="knowing-that-modifications-of-memory-is-consistently-observed" href="#knowing-that-modifications-of-memory-is-consistently-observed">Knowing That Modifications of Memory is Consistently Observed</a></span>
</h4>



<p>
Different hardware architectures have different memory models. Some
architectures allows very aggressive reordering of memory accesses
while other architectures only reorder a few specific cases. Common to
all modern hardware is, however, that some type of reordering will
occur. When using locks to protect all memory accesses made from
multiple threads such reorderings will not be visible. The locking
primitives will ensure that the memory accesses will be ordered. When
using lock free algorithms one do however have to take this reordering
made by the hardware into account.
</p>

<p>
Hardware memory barriers or memory fences are instructions that can be
used to enforce order between memory accesses. Different hardware
architectures provide different memory barriers. Lock free algorithms
need to use memory barriers in order to ensure that memory accesses
are not reordered in such ways that the algorithm breaks down. Memory
barriers are also expensive instructions, so you typically want to
minimize the use of these instructions.
</p>





<h3>
<a name="Functionality-Used-to-Address-These-Problems"></a><span onMouseOver="document.getElementById('ghlink-functionality-used-to-address-these-problems-idm30062').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-functionality-used-to-address-these-problems-idm30062').style.visibility = 'hidden';"><span id="ghlink-functionality-used-to-address-these-problems-idm30062"></span><a class="title_link" name="functionality-used-to-address-these-problems" href="#functionality-used-to-address-these-problems">2.2 
          Functionality Used to Address These Problems</a></span>
</h3>



<p>
The "thread progress" functionality in the Erlang VM is used to
address these problems. The name "thread progress" was chosen since we
want to use it to determine when all threads in a set of threads have
made such progress so that two specific events have taken place for
all them.
</p>

<p>
The set of threads that we are interested in we call managed
threads. The managed threads are the only threads that we get any
information about. These threads <strong>have</strong> to frequently report
progress. Not all threads in the system are able to frequently report
progress. Such threads cannot be allowed in the set of managed threads
and are called unmanaged threads. An example of unmanaged threads are
threads in the async thread pool. Async threads can be blocked for
very long times and by this be prevented from frequently reporting
progress. Currently only scheduler threads and a couple of other
threads are managed threads.
</p>

<h4>
<a name="Functionality-Used-to-Address-These-Problems_Thread-Progress-Events"></a><span onMouseOver="document.getElementById('ghlink-thread-progress-events-idm30068').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-thread-progress-events-idm30068').style.visibility = 'hidden';"><span id="ghlink-thread-progress-events-idm30068"></span><a class="title_link" name="thread-progress-events" href="#thread-progress-events">Thread Progress Events</a></span>
</h4>



<p>
Any thread in the system may use the thread progress functionality in
order to determine when the following events have occurred at least
once in all managed threads:
</p>

<ul>
<li>
The thread has returned from other code to a known state in the
thread progress functionality, which is independent of any other
code. 

</li>
<li>
The thread has executed a full memory barrier.

</li>
</ul>
<p>
These events, of course, need to occur ordered to other memory
operations. The operation of determining this begins by initiating the
thread progress operation. The thread that initiated the thread
progress operation after this poll for the completion of the
operation. Both of these events must occur at least once <strong>after</strong> the
thread progress operation has been initiated, and at least once
<strong>before</strong> the operation has completed in each managed thread. This is
ordered using communication via memory which makes it possible to draw
conclusion about the memory state after the thread progress operation
has completed. Lets call the progress made from initiation to
comletion for "thread progress".
</p>

<p>
Assuming that the thread progress functionality is efficient, a lot of
algorithms can both be simplified and made more efficient than using
the first approach that comes to mind. A couple of examples follows.
</p>

<p>
By being able to determine when the first event above has occurred we
can easily know when all managed threads have completed accesses to a
data structure. This can be determined the following way. We have an
implementation of some functionality <span class="code">F</span> using a data structure
<span class="code">D</span>. The reference to <span class="code">D</span> is always looked up before <span class="code">D</span> is being
accessed, and the references to <span class="code">D</span> is always dropped before we leave
the code implementing <span class="code">F</span>. If we remove the possibility to look up <span class="code">D</span>
and then wait until the first event has occurred in all managed
threads, no managed threads can have any references to the data
structure <span class="code">D</span>. This could for example have been achieved by using
reference counting, but the cache line containing the reference
counter would in this case be ping ponged between all processors
accessing <span class="code">D</span> at every access.
</p>

<p>
By being able to determine when the second event has occurred it is
quite easy to do complex modifications of memory that needs to be seen
consistently by other threads without having to resort to locking. By
doing the modifications, then issuing a full memory barrier, then wait
until the second event has occurred in all managed threads, and then
publish the modifications, we know that all managed threads reading
this memory will get a consistent view of the modifications. Managed
threads reading this will not have to issue any extra memory barriers
at all.
</p>





<h3>
<a name="Implementation-of-the-Thread-Progress-Functionality"></a><span onMouseOver="document.getElementById('ghlink-implementation-of-the-thread-progress-functionality-idm30090').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-implementation-of-the-thread-progress-functionality-idm30090').style.visibility = 'hidden';"><span id="ghlink-implementation-of-the-thread-progress-functionality-idm30090"></span><a class="title_link" name="implementation-of-the-thread-progress-functionality" href="#implementation-of-the-thread-progress-functionality">2.3 
          Implementation of the Thread Progress Functionality</a></span>
</h3>



<h4>
<a name="Implementation-of-the-Thread-Progress-Functionality_Requirement-on-the-Implementation"></a><span onMouseOver="document.getElementById('ghlink-requirement-on-the-implementation-idm30093').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-requirement-on-the-implementation-idm30093').style.visibility = 'hidden';"><span id="ghlink-requirement-on-the-implementation-idm30093"></span><a class="title_link" name="requirement-on-the-implementation" href="#requirement-on-the-implementation">Requirement on the Implementation</a></span>
</h4>



<p>
In order to be able to determine when all managed threads have reached
the states that we are interested in we need to communicate between
all involved threads. We of course want to minimize this
communication.
</p>

<p>
We also want threads to be able to determine when thread progress has
been made relatively fast. That is we need to have some balance
between comunication overhead and time to complete the operation.
</p>



<h4>
<a name="Implementation-of-the-Thread-Progress-Functionality_API"></a><span onMouseOver="document.getElementById('ghlink-api-idm30098').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-api-idm30098').style.visibility = 'hidden';"><span id="ghlink-api-idm30098"></span><a class="title_link" name="api" href="#api">API</a></span>
</h4>



<p>
I will only present the most important functions in the API here.
</p>

<ul>
<li>
<span class="code">ErtsThrPrgrVal erts_thr_progress_later(void)</span> - Initiation of the
operation. The thread progress value returned can be used testing
for completion of the operation.

</li>
<li>
<span class="code">int erts_thr_progress_has_reached(ErtsThrPrgrVal val)</span> - Returns
a non zero value when we have reached the thread progress value
passed as argument. That is, when a non zero value is returned the
operation has completed.

</li>
</ul>
<p>
When a thread calls <span class="code">my_val = erts_thr_progress_later()</span> and waits for
<span class="code">erts_thr_progress_has_reached(my_val)</span> to return a non zero value it
knows that thread progress has been made.
</p>

<p>
While waiting for <span class="code">erts_thr_progress_has_reached()</span> to return a non
zero value we typically do not want to block waiting, but instead want
to continue working with other stuff. If we run out of other stuff to
work on we typically do want to block waiting until we have reached
the thread progress value that we are waiting for. In order to be able
to do this we provide functionality for waking up a thread when a
certain thread progress value has been reached:
</p>

<ul><li>
<span class="code">void erts_thr_progress_wakeup(ErtsSchedulerData *esdp,
ErtsThrPrgrVal val)</span> - Request wake up. The calling thread will be
woken when thread progress has reached val. 

</li></ul>
<p>
Managed threads frequently need to update their thread progress by
calling the following functions:
</p>

<ul>
<li>
<span class="code">int erts_thr_progress_update(ErtsSchedulerData *esdp)</span> - Update
thread progress. If a non zero value is returned
<span class="code">erts_thr_progress_leader_update()</span> has to be called without any
locks held.

</li>
<li>
<span class="code">int erts_thr_progress_leader_update(ErtsSchedulerData *esdp)</span> -
Leader update thread progress.

</li>
</ul>
<p>
Unmanaged threads can delay thread progress being made:
</p>

<ul>
<li>
<span class="code">ErtsThrPrgrDelayHandle erts_thr_progress_unmanaged_delay(void)</span> -
Delay thread progress.

</li>
<li>
<span class="code">void erts_thr_progress_unmanaged_continue(ErtsThrPrgrDelayHandle
handle)</span> - Let thread progress continue.

</li>
</ul>
<p>
Scheduler threads can schedule an operation to be executed by the
scheduler itself when thread progress has been made:
</p>

<ul><li>
<span class="code">void erts_schedule_thr_prgr_later_op(void (*funcp)(void *), void
*argp, ErtsThrPrgrLaterOp *memp)</span> - Schedule a call to <span class="code">funcp</span>. The
call <span class="code">(*funcp)(argp)</span> will be executed when thread progress has been
made since the call to <span class="code">erts_schedule_thr_prgr_later_op()</span> was
made.

</li></ul>


<h4>
<a name="Implementation-of-the-Thread-Progress-Functionality_Implementation"></a><span onMouseOver="document.getElementById('ghlink-implementation-idm30135').style.visibility = 'visible';" onMouseOut="document.getElementById('ghlink-implementation-idm30135').style.visibility = 'hidden';"><span id="ghlink-implementation-idm30135"></span><a class="title_link" name="implementation" href="#implementation">Implementation</a></span>
</h4>



<p>
In order to determine when the events has happened we use a global
counter that is incremented when all managed threads have called
<span class="code">erts_thr_progress_update()</span> (or <span class="code">erts_thr_progress_leader_update()</span>).
This could naively be implemented using a "thread confirmed" counter.
This would however cause an explosion of communication where all
involved processors would need to communicate with each other at each
update.
</p>

<p>
Instead of confirming at a global location each thread confirms that
it accepts in increment of the global counter in its own cache
line. These confirmation cache lines are located in sequence in an
array, and each confirmation cache line will only be written by one
and only one thread. One of the managed threads always have the leader
responsibility. This responsibility may jump between threads, but as
long as there are some activity in the system always one of them will
have the leader responsibility. The thread with the leader
responsibility will call <span class="code">erts_thr_progress_leader_update()</span> which
will check that all other threads have confirmed an increment of the
global counter before doing the increment of the global counter. The
leader thread is the only thread reading the confirmation cache
lines.
</p>

<p>
Doing it this way we will get a communication pattern of information
going from the leader thread out to all other managed threads and then
back from the other threads to the leader thread. This since only the
leader thread will write to the global counter and all other threads
will only read it, and since each confirmation cache lines will only
be written by one specific thread and only read by the leader
thread. When each managed thread is distributed over different
processors, the communication between processors will be a reflection
of this communication pattern between threads.
</p>

<p>
The value returned from <span class="code">erts_thr_progress_later()</span> equals the, by
this thread, latest confirmed value plus two. The global value may be
latest confirmed value or latest confirmed value minus one. In order
to be certain that all other managed threads actually will call
<span class="code">erts_thr_progress_update()</span> at least once before we reach the value
returned from <span class="code">erts_thr_progress_later()</span>, the global counter plus one
is not enough. This since all other threads may already have confirmed
current global value plus one at the time when we call
<span class="code">erts_thr_progress_later()</span>. They are however guaranteed not to have
confirmed global value plus two at this time.
</p>

<p>
The above described implementation more or less minimizes the
comunication needed before we can increment the global counter. The
amount of communication in the system due to the thread progress
functionality however also depend on the frequency with which managed
threads call <span class="code">erts_thr_progress_update()</span>. Today each scheduler thread
calls <span class="code">erts_thr_progress_update()</span> more or less each time an Erlang
process is scheduled out. One way of further reducing communication
due to the thread progress functionality is to only call
<span class="code">erts_thr_progress_update()</span> every second, or third time an Erlang
process is scheduled out, or even less frequently than that. However,
by doing updates of thread progress less frequently all operations
depending on the thread progress functionality will also take a longer
time.
</p>

<h5>
<a name="Implementation-of-the-Thread-Progress-Functionality_Implementation_Delay-of-Thread-Progress-by-Unmanaged-Threads"></a>Delay of Thread Progress by Unmanaged Threads</h5>



<p>
In order to implement delay of thread progress from unmanaged threads
we use two reference counters. One being <span class="code">current</span> and one being
<span class="code">waiting</span>. When an unmanaged thread wants to delay thread progress it
increments <span class="code">current</span> and gets a handle back to the reference counter
it incremented. When it later wants to enable continuation of thread
progress it uses the handle to decrement the reference counter it
previously incremented.
</p>

<p>
When the leader threads is about to increment the global thread
progress counter it verifies that the <span class="code">waiting</span> counter is zero before
doing so. If not zero, the leader isn't allowed to increment the
global counter, and needs to wait before it can do this. When it is
zero, it swaps the <span class="code">waiting</span> and <span class="code">current</span> counters before increasing
the global counter. From now on the new <span class="code">waiting</span> counter will
decrease, so that it eventually will reach zero, making it possible to
increment the global counter the next time. If we only used one
reference counter it would potentially be held above zero for ever by
different unmanaged threads.
</p>

<p>
When an unmanaged thread increment the <span class="code">current</span> counter it will not
prevent the next increment of the global counter, but instead the
increment after that. This is sufficient since the global counter
needs to be incremented two times before thread progress has been
made. It is also desirable not to prevent the first increment, since
the likelihood increases that the delay is withdrawn before any
increment of the global counter is delayed. That is, the operation
will cause as little disruption as possible.
</p>

<p>
However, this feature of delaying thread progress from unmanaged
threads should preferably be used as little as possible, since heavy
use of it will cause contention on the reference counter cache
lines. The functionality is however very useful in code which normally
only executes in managed threads, but which may under some infrequent
circumstances be executed in other threads.
</p>



<h5>
<a name="Implementation-of-the-Thread-Progress-Functionality_Implementation_Overhead"></a>Overhead</h5>



<p>
The overhead caused by the thread progress functionality is more or
less fixed using the same amount of schedulers regardless of the
number of uses of the functionality. Already today quite a lot of
functionality use it, and we plan to use it even more. When rewriting
old implementations of ERTS internal functionality to use the thread
progress functionality, this implies removing communication in the old
implementation. Otherwise it is simply no point rewriting the old
implementation to use the thread progress functionality. Since the
thread progress overhead is more or less fixed, the rewrite will cause
a reduction of the total communication in the system.
</p>

<h5>
<a name="Implementation-of-the-Thread-Progress-Functionality_Implementation_Overhead_An-Example"></a>An Example</h5>



<p>
The main structure of an ETS table was originally managed using
reference counting. Already a long time ago we replaced this strategy
since the reference counter caused contention on each access of the
table. The solution used was to schedule "confirm deletion" jobs on
each scheduler in order to know when it was safe to deallocate the
table structure of a removed table. These confirm deletion jobs needed
to be allocated. That is, we had to allocate and deallocate as many
blocks as schedulers in order to deallocate one block. This of course
was a quite an expensive operation, but we only needed to do this once
when removing a table. It was more important to get rid of the
contention on the reference counter which was present on every
operation on the table.
</p>

<p>
When the thread progress functionality had been introduced, we could
remove the code implementing the "confirm deletion" jobs, and then
just schedule a thread progress later operation which deallocates the
structure. Besides simplifying the code a lot, we got an increase of
more than 10% of the number of transactions per second handled on a
mnesia tpcb benchmark executing on a quad core machine.
</p>








</div>
<div class="footer">
<hr>
<p>Copyright © 1997-2024 Ericsson AB. All Rights Reserved.</p>
</div>
</div>
</div>
<script type="text/javascript">window.__otpTopDocDir = '../../js/';</script><script type="text/javascript" src="../../js/highlight.js"></script>
</body>
</html>
